## create_lexicon.py

create_lexicon.py is a script for processing sentence corpora in the format given in sample_parsed_sentences.
Inputting the file 'sample_parsed_sentences.json' created the file 'sample_lexicon.json'

The input is parsed according to three nested classes defined in input_classes.py: SentenceCorpus, Sentence, and Token
Three functions are used to convert this data into the nested classes defined in output_classes.py: Lexicon, Entry, and Wordform
Each Token object is processed by the add_entry function, which assigns the Token's features as a new lemma, creating an Entry object (including the first wordform, added as a Wordform object)
If the lemma already exists, then the Token is processed by the update_entry function, which adds new Wordform objects accordingly
Frequency data is updated within the functions.
An extra function, parse_feats, converts the string of features and converts them to dictionary items, which are then assigned to the Wordform object

## Usage

Save the input file in the same directory as the script, and define the input file in line 9.
Define the output filename in line 10.

## Configuration

Arguments can be added to model_dump_json in line 112 to exclude None values or certain features from the output
(see Pydantic docs, https://docs.pydantic.dev/latest/api/base_model/#pydantic.main.BaseModel.model_dump_json)
Currently, each Wordform is created with all available features, even for parts of speech that do not need them (e.g. verbs have 'case': '')
A configuration file could be created in the pipeline to define a schema for these features.
This configuration file could also define which parts of speech to skip; currently the script skips number and punctuation tokens by default (line 134)

## Place within a production environment

The previous stage to the lexicon creation is the parsing of sentences in a corpus. The sample output data of this stage seems to be creating errors (see below).
This file may be generated from a single corpus; there may be multiple corpora being used to generate the lexicon, and therefore a later stage in the pipeline should update the frequencies generated here, as well as generating relative frequency accordingly.
Although Pydantic ensures strict data validation, the JSON file generated by this script should be validated by the next script in the pipeline.
Amazon Web Services could be used to host the input and output data.

## Errors in sample data

Unlikely frequency figures in the sample output led me to the identification of errors in the data sample provided.
Some tokens are duplicated or triplicated. E.g., the first set of tokens finish at character 251 (before the end of the string), and begin again at 0.

The following token, for instance, appears twice (l. 18, l. 338), despite appearing in the first sentence only once:

```
"id": "2",
"text": "жылы",
"lemma": "жыл",
"pos": "NOUN",
"pos_finegrained": "n",
"feats": "Case=Nom|Number[psor]=Plur,Sing|Person[psor]=3",
"start_char": "5",
"end_char": "9"
```

This inflates the frequency count in the output of the script. Future iterations of this script should check to prevent adding duplicates;
however, the solution must prioritize fixing the script which generated the sample data.
As most (if not all) of these errors appear around slashes or quotation marks, I suspect it is something to do with how the parsing script handles escape characters and/or string delimiters.
